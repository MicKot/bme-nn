{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20180110 - My Neural Network Lab notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Keras include sample datasets, e.g. movie revies from IMDB\n",
    "\n",
    "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). \n",
    "\n",
    "Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "This dataset comes preloaded with Keras, so one simple command will get us training and testing data. There is a parameter for how many words we want to look at. We've set it at 1000, but feel free to experiment.\n",
    "\n",
    "Usage:\n",
    "\n",
    "```python\n",
    "from keras.datasets import imdb\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=None,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)\n",
    "```\n",
    "\n",
    "Returns:\n",
    "\n",
    "2 tuples:\n",
    "\n",
    "x_train, x_test: list of sequences, which are lists of indexes (integers). If the num_words argument was specific, the maximum possible index value is num_words-1. If the maxlen argument was specified, the largest possible sequence length is maxlen.\n",
    "\n",
    "y_train, y_test: list of integer labels (1 or 0).\n",
    "\n",
    "\n",
    "Arguments:\n",
    "path: if you do not have the data locally (at '~/.keras/datasets/' + path), it will be downloaded to this location.\n",
    "\n",
    "num_words: integer or None. Top most frequent words to consider. Any less frequent word will appear as oov_char value in the sequence data.\n",
    "\n",
    "skip_top: integer. Top most frequent words to ignore (they will appear as oov_char value in the sequence data).\n",
    "\n",
    "maxlen: int. Maximum sequence length. Any longer sequence will be truncated.\n",
    "\n",
    "seed: int. Seed for reproducible data shuffling.\n",
    "\n",
    "start_char: int. The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\n",
    "\n",
    "oov_char: int. words that were cut out because of the num_words or skip_top limit will be replaced with this character.\n",
    "\n",
    "index_from: int. Index actual words with this index and higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of data vector\n",
      "25000\n",
      "Shape of data vectors:\n",
      "(25000,)\n",
      "(25000,)\n",
      "\n",
      "First element of data vectors\n",
      "[2, 2, 2, 2, 2, 530, 973, 1622, 1385, 65, 458, 2, 66, 2, 2, 173, 2, 256, 2, 2, 100, 2, 838, 112, 50, 670, 2, 2, 2, 480, 284, 2, 150, 2, 172, 112, 167, 2, 336, 385, 2, 2, 172, 2, 1111, 2, 546, 2, 2, 447, 2, 192, 50, 2, 2, 147, 2, 2, 2, 2, 2, 1920, 2, 469, 2, 2, 71, 87, 2, 2, 2, 530, 2, 76, 2, 2, 1247, 2, 2, 2, 515, 2, 2, 2, 626, 2, 2, 2, 62, 386, 2, 2, 316, 2, 106, 2, 2, 2, 2, 2, 480, 66, 2, 2, 2, 130, 2, 2, 2, 619, 2, 2, 124, 51, 2, 135, 2, 2, 1415, 2, 2, 2, 2, 215, 2, 77, 52, 2, 2, 407, 2, 82, 2, 2, 2, 107, 117, 2, 2, 256, 2, 2, 2, 2, 2, 723, 2, 71, 2, 530, 476, 2, 400, 317, 2, 2, 2, 2, 1029, 2, 104, 88, 2, 381, 2, 297, 98, 2, 2, 56, 2, 141, 2, 194, 2, 2, 2, 226, 2, 2, 134, 476, 2, 480, 2, 144, 2, 2, 2, 51, 2, 2, 224, 92, 2, 104, 2, 226, 65, 2, 2, 1334, 88, 2, 2, 283, 2, 2, 2, 113, 103, 2, 2, 2, 2, 2, 178, 2]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=2000,skip_top=50)\n",
    "\n",
    "print('\\nLength of data vector')\n",
    "print(len(x_train))\n",
    "\n",
    "print('Shape of data vectors:')\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "print('\\nFirst element of data vectors')\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data encoding\n",
    "Converting the input vectors into one-hot encoded vectors. \n",
    "\n",
    "0\t00000001\n",
    "\n",
    "1\t00000010\n",
    "\n",
    "2\t00000100\n",
    "\n",
    "https://en.wikipedia.org/wiki/One-hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2000)\n",
      "(25000, 2000)\n",
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 2)\n",
    "y_test = keras.utils.to_categorical(y_test, 2)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 512)               1024512   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 1,025,538\n",
      "Trainable params: 1,025,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# sequential model architecture with one layer of length 100\n",
    "num_classes=2\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=2000))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model using categorical_crossentropy loss, and rmsprop optimizer.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 18s 726us/step - loss: 0.3640 - acc: 0.8537 - val_loss: 0.3097 - val_acc: 0.8737\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 18s 708us/step - loss: 0.3080 - acc: 0.8830 - val_loss: 0.3234 - val_acc: 0.8743\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 18s 715us/step - loss: 0.2959 - acc: 0.8920 - val_loss: 0.3169 - val_acc: 0.8766\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 18s 717us/step - loss: 0.2903 - acc: 0.8935 - val_loss: 0.3378 - val_acc: 0.8689\n",
      "Epoch 5/10\n",
      " 2176/25000 [=>............................] - ETA: 14s - loss: 0.2944 - acc: 0.9030"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=16,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test), \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quick visualization of model training history')\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('accuracy: %.2f%%' % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Take a look at model.predict function\n",
    "\n",
    "predict(self, x, batch_size=None, verbose=0, steps=None)\n",
    "\n",
    "Generates output predictions for the input samples.\n",
    "\n",
    "The input samples are processed batch by batch.\n",
    "\n",
    "Arguments\n",
    "\n",
    "x: the input data, as a Numpy array.\n",
    "\n",
    "batch_size: Integer. If unspecified, it will default to 32.\n",
    "\n",
    "verbose: verbosity mode, 0 or 1.\n",
    "\n",
    "steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None.\n",
    "\n",
    "Returns\n",
    "\n",
    "A Numpy array of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - correct the output of predict function\n",
    "\n",
    "prediction = model.predict(x_test)\n",
    "print(prediction)\n",
    "\n",
    "print(abs(prediction-y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More advance exmaple\n",
    "\n",
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
